{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd42a3a-e12c-4460-ae78-e4dbe701c9e4",
   "metadata": {},
   "source": [
    "Generación de texto con un RNN \n",
    "https://www.tensorflow.org/text/tutorials/text_generation?hl=es-419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b0e1d76-1d5c-46cd-b6c5-f5e0a7ac3a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e751e158-e251-4c67-abde-a3519988c838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3e6d17-a8b5-45d5-b804-7e1712a4abec",
   "metadata": {},
   "source": [
    "Leer los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63ac837-4ad2-4905-b594-f17f185d3091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c3d705-b556-428d-b752-717adb0b69bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea524dce-25d9-47f6-8c1a-1885627ffa9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58073b1d-9869-41b3-beef-280ae6096dc1",
   "metadata": {},
   "source": [
    "Procesar el texto\n",
    "Vectorizar el texto\n",
    "Antes del entrenamiento, debe convertir las cadenas en una representación numérica.\n",
    "\n",
    "La capa tf.keras.layers.StringLookup puede convertir cada carácter en un ID numérico. Solo necesita que el texto se divida en tokens primero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4b1821-40ca-4c81-9cd1-f1711d663e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1181def4-ff3f-44e5-83e2-4e70b8d0900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a40306-7fb0-4fcf-9e83-8390432f9591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ae1f2-31e5-4372-a80b-2c34cdd5b0a5",
   "metadata": {},
   "source": [
    "Dado que el objetivo de este tutorial es generar texto, también será importante invertir esta representación y recuperar cadenas legibles por humanos a partir de ella. Para esto, puede usar tf.keras.layers.StringLookup(..., invert=True) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e28eb85-924d-4e10-92c3-35f5f9cb2115",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02929e7e-c69f-4dcc-a686-230e61c235f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d835b1e-0b4c-488b-850d-a80348d0c262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea5fa193-d621-4c95-b750-981ac21c0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4327dae-b058-4b98-beab-620f08eb7401",
   "metadata": {},
   "source": [
    "La tarea de predicción\n",
    "Dado un carácter, o una secuencia de caracteres, ¿cuál es el próximo carácter más probable? Esta es la tarea para la que está entrenando al modelo. La entrada al modelo será una secuencia de caracteres, y usted entrena el modelo para predecir la salida: el siguiente carácter en cada paso de tiempo.\n",
    "\n",
    "Dado que los RNN mantienen un estado interno que depende de los elementos vistos anteriormente, dados todos los caracteres computados hasta este momento, ¿cuál es el siguiente carácter?\n",
    "\n",
    "Cree ejemplos y objetivos de capacitación\n",
    "A continuación, divida el texto en secuencias de ejemplo. Cada secuencia de entrada contendrá caracteres seq_length del texto.\n",
    "\n",
    "Para cada secuencia de entrada, los objetivos correspondientes contienen la misma longitud de texto, excepto que se desplaza un carácter a la derecha.\n",
    "\n",
    "Así que divide el texto en partes de seq_length+1 . Por ejemplo, digamos que seq_length es 4 y nuestro texto es \"Hola\". La secuencia de entrada sería \"Hell\" y la secuencia de destino \"ello\".\n",
    "\n",
    "Para hacer esto, primero use la función tf.data.Dataset.from_tensor_slices para convertir el vector de texto en una secuencia de índices de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e318441-cb48-431f-ad0c-3ca51a7e149a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d9f71f0-6882-40d8-9119-801801ef4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beedbc9d-2bf1-4596-98c1-0411e259a028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a37fe684-86ff-4f82-997c-6f573f1c7f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eb2515c-8a33-41a0-bf05-70f73f397cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6716fc9f-b007-4aab-be53-fc54baa26ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70514f1c-c619-44a0-bedf-f0556057d764",
   "metadata": {},
   "source": [
    "Para el entrenamiento, necesitará un conjunto de datos de (input, label) pares. Donde input y label son secuencias. En cada paso de tiempo, la entrada es el carácter actual y la etiqueta es el siguiente carácter.\n",
    "\n",
    "Aquí hay una función que toma una secuencia como entrada, la duplica y la cambia para alinear la entrada y la etiqueta para cada paso de tiempo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a7e646d-9fbd-4b25-894f-83aaa29b0855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a99e668-2987-428e-bc3e-37859242ca63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "351c1efc-4a83-410a-a859-40458cb92397",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0780e26e-3082-44ee-8882-020a0fe0be85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f160aaf1-5b4a-4eb1-9c08-583e39c223ba",
   "metadata": {},
   "source": [
    "Crear lotes de entrenamiento\n",
    "Usó tf.data para dividir el texto en secuencias manejables. Pero antes de introducir estos datos en el modelo, debe mezclar los datos y empaquetarlos en lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c303c399-0841-4613-acbc-480c79a0e0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ccaf45-e77c-43d0-a264-edf67734500c",
   "metadata": {},
   "source": [
    "construir el modelo\n",
    "Esta sección define el modelo como una subclase keras.Model (para obtener más información, consulte Creación de nuevas capas y modelos a través de subclases ).\n",
    "\n",
    "Este modelo tiene tres capas:\n",
    "\n",
    "tf.keras.layers.Embedding : La capa de entrada. Una tabla de búsqueda entrenable que asignará cada ID de carácter a un vector con dimensiones embedding_dim ;\n",
    "tf.keras.layers.GRU : un tipo de RNN con units=rnn_units (también puede usar una capa LSTM aquí).\n",
    "tf.keras.layers.Dense : la capa de salida, con salidas vocab_size . Produce un logit por cada carácter del vocabulario. Estos son el log-verosimilitud de cada carácter según el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d92d902-eec3-43a0-8214-7b5324a1ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04029b91-ae79-4a5e-9f9d-836500f82a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae32414b-4c0b-44dd-92c0-2f969ae831ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab0f6d-c1b7-41ee-9803-c1080e84bafc",
   "metadata": {},
   "source": [
    "Prueba el modelo\n",
    "Ahora ejecute el modelo para ver que se comporta como se esperaba.\n",
    "\n",
    "Primero verifique la forma de la salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba7e17eb-9fe7-43d1-9a0e-6a9eac532020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[21 18 32 ... 55 55 40]\n",
      " [14 32 33 ...  9  1  1]\n",
      " [57  7  2 ... 14 15 31]\n",
      " ...\n",
      " [41 44  2 ... 48 58  2]\n",
      " [48 51 51 ... 47 48 52]\n",
      " [16 51 40 ...  2 41 48]], shape=(64, 100), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[18 32 32 ... 55 40 57]\n",
      " [32 33 22 ...  1  1 26]\n",
      " [ 7  2 22 ... 15 31 14]\n",
      " ...\n",
      " [44  2 45 ... 58  2 46]\n",
      " [51 51  2 ... 48 52  2]\n",
      " [51 40 60 ... 41 48 59]], shape=(64, 100), dtype=int64)\n",
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    print(input_example_batch)\n",
    "    print(target_example_batch)\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffdeba52-5076-4138-bae7-8b683bd5aed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  16896     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 multiple                  3938304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea3af47a-6b2b-4d40-9fd6-e100a191018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5450bbdf-9ef0-4fe3-8185-8fc41b6843a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 38, 63, 59, 50,  9, 56, 41, 21, 20, 23, 63, 57,  6,  0, 12, 48,\n",
       "       44, 53, 51, 34, 13, 47, 46, 43, 20, 29, 46, 46, 26, 12, 14, 52, 16,\n",
       "       57,  5, 16, 51,  5, 50, 37, 23, 25, 50, 60, 53, 60, 41, 39,  0, 50,\n",
       "       54, 25, 39, 56, 61, 63, 27, 30, 29,  2, 13,  6, 42,  0, 40, 33, 17,\n",
       "        7, 12,  0, 64, 64, 53, 62, 53, 33, 63, 45,  9, 30, 20, 48, 16, 20,\n",
       "       30, 49, 65, 65, 18, 52, 47, 41, 12, 15, 21, 32, 42, 38, 56],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc7d178f-a2b7-4238-aee9-ad4d1f8b4e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b\"HESS OF YORK:\\nWhat should you fear?\\n'Tis nothing but some bond, that he is enter'd into\\nFor gay appa\"\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"YYxtk.qbHGJxr'[UNK];ienlU?hgdGPggM;AmCr&Cl&kXJLkunubZ[UNK]koLZqvxNQP ?'c[UNK]aTD,;[UNK]yynwnTxf.QGiCGQjzzEmhb;BHScYq\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93767396-a8e8-411b-9476-8cb6f456e245",
   "metadata": {},
   "source": [
    "entrenar al modelo\n",
    "En este punto, el problema se puede tratar como un problema de clasificación estándar. Dado el estado anterior de RNN y la entrada de este paso de tiempo, prediga la clase del siguiente carácter.\n",
    "\n",
    "Adjunte un optimizador y una función de pérdida\n",
    "La función de pérdida estándar tf.keras.losses.sparse_categorical_crossentropy funciona en este caso porque se aplica en la última dimensión de las predicciones.\n",
    "\n",
    "Debido a que su modelo devuelve logits, debe configurar el indicador from_logits ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e27f665-122e-4450-b2ae-4ca5ea5ca2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6df5ea2a-5e75-4da0-ac9e-495c60759739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1899085, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9a6ccc1-3a5b-466d-b2a5-aa71b3f13a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.01675"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf41b820-2c4c-4f65-a32e-5e30a39f7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543fc143-46f0-4f34-a9ee-d13118401c8d",
   "metadata": {},
   "source": [
    "Configurar puntos de control\n",
    "Use un tf.keras.callbacks.ModelCheckpoint para asegurarse de que los puntos de control se guarden durante el entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85f4a1cb-907e-4db1-a59c-3f1d8ff7ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "762d6be7-7e8a-41a5-b277-ff025c64c4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 9s 39ms/step - loss: 2.7074\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 7s 41ms/step - loss: 1.9830\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.7067\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 7s 39ms/step - loss: 1.5460\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 7s 39ms/step - loss: 1.4468\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 7s 37ms/step - loss: 1.3791\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 7s 37ms/step - loss: 1.3264\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 7s 37ms/step - loss: 1.2822\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.2410\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 7s 37ms/step - loss: 1.2018\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.1623\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.1211\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 1.0784\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 7s 37ms/step - loss: 1.0326\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 7s 37ms/step - loss: 0.9848\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 7s 40ms/step - loss: 0.9343\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 7s 37ms/step - loss: 0.8833\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 7s 37ms/step - loss: 0.8304\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 7s 36ms/step - loss: 0.7805\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 7s 38ms/step - loss: 0.7339\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81cd63-5f26-411b-84f2-77689c1a0706",
   "metadata": {},
   "source": [
    "Generar texto\n",
    "La forma más sencilla de generar texto con este modelo es ejecutarlo en un bucle y realizar un seguimiento del estado interno del modelo a medida que lo ejecuta.\n",
    "Cada vez que llama al modelo, pasa un texto y un estado interno. El modelo devuelve una predicción para el siguiente carácter y su nuevo estado. Vuelva a pasar la predicción y el estado para continuar generando texto.\n",
    "\n",
    "Lo siguiente hace una predicción de un solo paso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "34c00cb8-470c-418a-a4c6-2686620cfc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4bd9f47-88d3-4b5a-9143-03ba462bf65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60ec8d-3bdf-4a7a-b696-5256d3b6e109",
   "metadata": {},
   "source": [
    "Ejecútelo en un bucle para generar algo de texto. Mirando el texto generado, verá que el modelo sabe cuándo usar mayúsculas, hacer párrafos e imita un vocabulario de escritura similar al de Shakespeare. Con el pequeño número de épocas de entrenamiento, aún no ha aprendido a formar oraciones coherentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5842839b-7548-43df-8c89-75928ddbefc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "In'tio, peace:\n",
      "I'll ha' my meet; and, by my tale, I will\n",
      "not turned and set for it.\n",
      "\n",
      "GLOUCESTER:\n",
      "I will, my diech-lying.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Well, thou, or when Tybalt, Let him in ut all?\n",
      "The mayor towards London they learning in your arms,\n",
      "But lets your slender wanton puts together\n",
      "And instruments of the masch. What is thy grining would,\n",
      "And shame so pardon then of Junies,\n",
      "Are done, but long to hear a worthy nurse,\n",
      "How he appears dicarred wnought,\n",
      "I will make foul grief broke. He mean to see your heart;\n",
      "Whereto we drink the ear that contradict\n",
      "Is a house and Birbshal quive or other\n",
      "The man not with myself with my delivery\n",
      "As they meen and medely draws, in a commoster\n",
      "Toward the nobilit in my misery, and\n",
      "your mother had been blood, our caminis lords.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "She cross, my lord of Lucio.\n",
      "\n",
      "LUCIO:\n",
      "Fear esemes we stoop.\n",
      "\n",
      "MENENIUS:\n",
      "Why then no more?\n",
      "She shall not brought the power of unchrobed;\n",
      "Though his victorious regar to their princes,\n",
      "now power: the magam warriors, steel as our \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.2107207775115967\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df47aef4-a172-436f-9ef9-8550908eb0ea",
   "metadata": {},
   "source": [
    "Lo más fácil que puede hacer para mejorar los resultados es entrenarlo durante más tiempo (pruebe EPOCHS = 30 ).\n",
    "\n",
    "También puede experimentar con una cadena de inicio diferente, intente agregar otra capa RNN para mejorar la precisión del modelo o ajustar el parámetro de temperatura para generar predicciones más o menos aleatorias.\n",
    "\n",
    "Si desea que el modelo genere texto más rápido , lo más fácil que puede hacer es generar el texto por lotes. En el siguiente ejemplo, el modelo genera 5 salidas aproximadamente en el mismo tiempo que se tardó en generar 1 arriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb3cd561-8ce8-4043-a611-6ae0ffb379a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nThe mayor in hand, as he will desire.\\n\\nPETRUCHIO:\\nI know not what to say that she is fight\\nThat hads more matter to shoce me for't.\\n\\nPETRUCHIO:\\nI'll be a mag--but still with self-wealth for a dear;\\nSo doth contrard my true lips,\\nWhipping in our countrymen and now in the dispatch:\\n'Thus stand but that which shall be consumed with curs,\\nStabb'd by our liege, with thy sorrow,\\nuncantly and outswands unto her face, and who\\nTo stand me: throw the time o' the\\nking; what, I propees a bawd, anly night\\nHad rather be ourself.\\n\\nLORD STANLAY:\\nIf thou be minex, being good speech; tell me\\nThe fault's of honour that my more disigns\\nWhose tatte's ease to shruk desolt to discupor.\\n\\nBUCKINGHAM:\\nDo not swear, and fair cousan will the king.\\n\\nRICHMOND:\\nWhy, this we long shall you stay time here,\\nAnd that thy polsces by Libut's blood.\\n\\nGREMIO:\\nHarsh he that both prince, and fight-door;\\nif thou hast stoop'd in my meed, their trades\\nWith duty takes.\\nFirst, here, sir.\\n\\nCORIOLANUS:\\nWho in mine eyes, peoples, th\"\n",
      " b\"ROMEO:\\nNot to the Truth, but cut my reputation\\nIf ne'er cannot amile your purpose;\\nWould yaugh, and will, in banishment in me;\\nFor in this deck'd, when I am in apples:\\nThere is my Childish queen; and now My fleet boss\\npled, to have no stain the thirs, the world, the fire,\\nOr any in their treasures not\\nAin every cannior lives but with such a miscain;\\nOr with his suits is, she would have made them up,\\nBe o'erta'-ne'er of me, and you shall compare\\nTo prove my true oaths.\\n\\nPOLIXENES:\\nWhy, she's a whit, nop nors.\\n\\nANTONIO:\\nHa's my good friend?\\n\\nPedant:\\nO Despite, master; and, by dost she live\\nNo bend of such a man would know his plain.\\nIf thou art deceived, or catches his body born.\\nThe pentle cury see more vow'd, why, you should\\nMake out of others which name me butchers! for therefore, I pretty us.\\n\\nBLUNT:\\nHe leads he smooth disdradue up this point.\\n\\nKING RICHARD II:\\nRichard? God hath her; and yours, play haging,\\nCome, muster nurs, and spur my men with directions, and\\nthe wisest thus, 'Come, lik\"\n",
      " b\"ROMEO:\\nWhy, how now, day! What, hath such a naturation,\\nWhich, like an officer, too much I call thee\\nIs thieves for their names of Lord Soldiers,\\nSee though the crown to them and will,\\nLike pardoning aff, the gods forsworn,\\nOur steen and mischief; the oracle is,\\nI will practise have I suppose to fall in a month\\n'Gainst Lucio, did I did call the prison?\\n\\nGREGORY:\\nNo? My woe woo'd, he will for thee to be gone?\\nLive well in veil'd my horse. When he did but a\\ncountetal, would all in law, Sicilias living.\\n\\nGLOUCESTER:\\nArgs to do that Juliet,\\nShall I soon cord other such a man?\\n\\nRICHARD:\\nNay, I'll not go to have one heat them in,\\nSaid to them. Yet peerer, humbly in our arms,\\nCommit the obedience and succeeding in the Volscian state,\\nWith opposite thereon,\\nMuch about my mistress. Sir William Richard's queen;\\nFor this will I did meer it me: I will die, I pray thee with him allow,\\nMy duty more much amplotiny whereof, as,\\nSo that he times, is now nor parasle.\\n\\nAUTOLYCUS:\\nI am a read of that brot wrath\"\n",
      " b\"ROMEO:\\nThe soul to the dead is just;\\nAnd she hootest was great king, and hearing her heart\\nTo hop their influences, they know not only he\\nWould they do meet the leaded earls no plays of me,\\nWhom they say the sleeping? We have done\\nAnd in my own knife I saw sweet report,\\nMay not show for the most day to-day.\\n\\nPAULINA:\\nI do, and give lectuent.\\n\\nLADY ANNE:\\nWhy, then the most shall come to France? 'twas well add\\nEnturious Duke of York long love; then let phy\\nsee this apparenced, and thy favour die shine him thanks;\\nSo owe, thou art, Signior Valentio's point.\\n\\nRICHARD:\\nNow, Sisor, Sir William Richard sleeping Counsel:\\nHonest death, my lord; I warrant you. He that will,\\nI sent thee two suit.\\n\\nCAMILLO:\\nI pray you?\\n\\nMARIANA:\\nPrinces and nox\\nThou hast the mother and to love my keeping cue.\\n\\nPETER:\\nTouching his brother pardon to my wife!\\nSpeak supper, that he would not show your grace\\nMy dance, according to this trunk. But what,\\nPut not this borne be his vice, being me\\nShe is your dream Angelo:\\nO neve\"\n",
      " b\"ROMEO:\\nThe maning of the honour and bedstake\\nWith any kind of love. Remious death.\\n\\nDUKE VINCENTIO:\\nHe coinse that I enforce thy mistress of your petition;\\nAnd this, I hope, that stabr'd it more\\nOf noses.\\n\\nAUFIDIUS:\\nThey stay me, I say, I pray you.\\n\\nSecond Lett:\\nNo, not what answer no more! and yet she should be adorn;\\nFor this one beats raging to hurt and scorn'd,\\nNot sound, to lamentation.\\n\\nVOLUMNIA:\\nLeave been would kill my pals!\\n\\nCLAUDIO:\\nWhy, then she will substribut.\\n\\nWARWICK:\\nTut, tut!\\nI cannot say 'O, be it made betwe no other\\nWithin amorots: Deching clouds,\\nBut let us mistoke the Duke of Extranghion\\nTo show myself a brave father, now;\\nBack, learning here in Christendom, whom my father\\nDepose him spent born to see me, and now\\nMore fleerich throne suspicion: shall we toward the noke\\nMust strength castle have here; this last discourse had not\\nso socret him seen those corys\\nShould tell the wolf that will fescy you are example.\\nAnd, in good time, how should they prove the metion\\nWill res\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 3.2604644298553467\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30d0e69-8aab-43b1-acb1-7d1c9027c9c9",
   "metadata": {},
   "source": [
    "Exportar el generador\n",
    "Este modelo de un solo paso se puede guardar y restaurar fácilmente, lo que le permite usarlo en cualquier lugar donde se acepte un tf.saved_model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d96f1f3-bc05-4815-88a1-725849624215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x000001CAD581AC40>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a47f5a0d-c774-4b59-b327-936005b17bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "I do desire to him.\n",
      "To whom Decoursessing herself twic, thou else,\n",
      "The precious better death had ga\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298f237-1f95-4f0f-bdfc-a056873e033e",
   "metadata": {},
   "source": [
    "Avanzado: Entrenamiento Personalizado\n",
    "El procedimiento de entrenamiento anterior es simple, pero no le da mucho control. Utiliza el maestro forzado que evita que las malas predicciones se retroalimenten al modelo, por lo que el modelo nunca aprende a recuperarse de los errores.\n",
    "\n",
    "Entonces, ahora que ha visto cómo ejecutar el modelo manualmente, ahora implementará el ciclo de entrenamiento. Esto brinda un punto de partida si, por ejemplo, desea implementar el aprendizaje del plan de estudios para ayudar a estabilizar la salida de bucle abierto del modelo.\n",
    "\n",
    "La parte más importante de un ciclo de entrenamiento personalizado es la función de paso de entrenamiento.\n",
    "\n",
    "Use tf.GradientTape para rastrear los degradados. Puede obtener más información sobre este enfoque leyendo la guía de ejecución ansiosa .\n",
    "\n",
    "El procedimiento básico es:\n",
    "\n",
    "Ejecute el modelo y calcule la pérdida bajo un tf.GradientTape .\n",
    "Calcule las actualizaciones y aplíquelas al modelo utilizando el optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "849ab059-260a-4299-a58b-39c2519dfc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cf8ffb-16c4-4bd2-9d4e-2bccfbb4c99b",
   "metadata": {},
   "source": [
    "La implementación anterior del método train_step sigue las convenciones train_step de Keras . Esto es opcional, pero le permite cambiar el comportamiento del paso de tren y seguir usando los métodos Model.compile y Model.fit de keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4bc327c-5fdd-45c8-bb9b-f9008af3094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dcc434d6-054a-43ff-901d-dee350a4bd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 8s 35ms/step - loss: 2.7003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cc90e5da90>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc2966-79b4-4001-8a78-ec2d548a3fdd",
   "metadata": {},
   "source": [
    "O si necesita más control, puede escribir su propio ciclo de entrenamiento personalizado completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e06723d7-7c7d-44db-8147-2cda58cf475d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.1523\n",
      "Epoch 1 Batch 50 Loss 2.0583\n",
      "Epoch 1 Batch 100 Loss 1.9722\n",
      "Epoch 1 Batch 150 Loss 1.8533\n",
      "\n",
      "Epoch 1 Loss: 1.9768\n",
      "Time taken for 1 epoch 6.86 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 2 Batch 0 Loss 1.7950\n",
      "Epoch 2 Batch 50 Loss 1.7174\n",
      "Epoch 2 Batch 100 Loss 1.6798\n",
      "Epoch 2 Batch 150 Loss 1.6261\n",
      "\n",
      "Epoch 2 Loss: 1.6955\n",
      "Time taken for 1 epoch 6.49 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 3 Batch 0 Loss 1.5748\n",
      "Epoch 3 Batch 50 Loss 1.5589\n",
      "Epoch 3 Batch 100 Loss 1.5058\n",
      "Epoch 3 Batch 150 Loss 1.4958\n",
      "\n",
      "Epoch 3 Loss: 1.5367\n",
      "Time taken for 1 epoch 6.72 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 4 Batch 0 Loss 1.4303\n",
      "Epoch 4 Batch 50 Loss 1.4210\n",
      "Epoch 4 Batch 100 Loss 1.4102\n",
      "Epoch 4 Batch 150 Loss 1.3767\n",
      "\n",
      "Epoch 4 Loss: 1.4399\n",
      "Time taken for 1 epoch 6.41 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 5 Batch 0 Loss 1.3945\n",
      "Epoch 5 Batch 50 Loss 1.3484\n",
      "Epoch 5 Batch 100 Loss 1.3457\n",
      "Epoch 5 Batch 150 Loss 1.3786\n",
      "\n",
      "Epoch 5 Loss: 1.3726\n",
      "Time taken for 1 epoch 7.21 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 6 Batch 0 Loss 1.3347\n",
      "Epoch 6 Batch 50 Loss 1.3295\n",
      "Epoch 6 Batch 100 Loss 1.3143\n",
      "Epoch 6 Batch 150 Loss 1.3226\n",
      "\n",
      "Epoch 6 Loss: 1.3205\n",
      "Time taken for 1 epoch 6.44 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 7 Batch 0 Loss 1.2896\n",
      "Epoch 7 Batch 50 Loss 1.2733\n",
      "Epoch 7 Batch 100 Loss 1.2667\n",
      "Epoch 7 Batch 150 Loss 1.2379\n",
      "\n",
      "Epoch 7 Loss: 1.2759\n",
      "Time taken for 1 epoch 6.38 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 8 Batch 0 Loss 1.2385\n",
      "Epoch 8 Batch 50 Loss 1.2280\n",
      "Epoch 8 Batch 100 Loss 1.2301\n",
      "Epoch 8 Batch 150 Loss 1.2134\n",
      "\n",
      "Epoch 8 Loss: 1.2348\n",
      "Time taken for 1 epoch 6.55 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 9 Batch 0 Loss 1.1837\n",
      "Epoch 9 Batch 50 Loss 1.1830\n",
      "Epoch 9 Batch 100 Loss 1.2093\n",
      "Epoch 9 Batch 150 Loss 1.2124\n",
      "\n",
      "Epoch 9 Loss: 1.1939\n",
      "Time taken for 1 epoch 6.73 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 10 Batch 0 Loss 1.1222\n",
      "Epoch 10 Batch 50 Loss 1.1706\n",
      "Epoch 10 Batch 100 Loss 1.1724\n",
      "Epoch 10 Batch 150 Loss 1.2067\n",
      "\n",
      "Epoch 10 Loss: 1.1541\n",
      "Time taken for 1 epoch 7.13 sec\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    mean.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "    print(\"_\"*80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3117c5-1c93-441b-9114-ac3e25220211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
